% $Id:$
%
\documentclass[a4paper,10pt,twoside]{report}
%
\usepackage[english,dutch,ngerman,english]{babel}
\usepackage{microtype}
\usepackage{relsize} %for \textscale{.75}
\usepackage{booktabs}
\usepackage{varioref}
\usepackage[verbose]{geometry}
%
%\usepackage{tikz}
%\usetikzlibrary{shapes.geometric,shapes.arrows,decorations.pathmorphing}
%\usetikzlibrary{matrix,chains,scopes,positioning,arrows,fit}
%
% These three are together:
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[style=british]{csquotes}
%
\usepackage{color}
\usepackage{url}
\usepackage{sepnum}
\usepackage{appendix}
\usepackage{amssymb} %for twoheadrightarrow 
%
\usepackage{setspace}
\onehalfspacing
%
\usepackage{verbatim}
%
%http://tex.stackexchange.com/questions/18969/creating-a-zebra-effect-using-listings
\usepackage{lstlinebgrd}
%
% ---- Commands/definitions/&c.
%
%\makeatletter
\input{defs.tex}
\input{acronyms.tex}
\input{tikzdefs.tex}
%\makeatother
%
% Chapter/section style
\usepackage{pbstyle}
% ----

%%I'm afraid that if you look at a thing long enough, it loses
%% all of its meaning. --Andy Warhol

\begin{document}

\title{Wopr}
\author{Peter Berck}
\date{\today}

\maketitle

%% ----------------------------------------------------------------
%\makeemptypage
\clearpage
\tableofcontents

%% ---

\chapter{Intro}

\Wopr{} is a wrapper around the \knn{} classifier in \Timbl{},
offering word prediction and language modeling
functionalities. Trained on a text corpus, \wopr{} can predict missing
words, report perplexities at the word level and the text level, and
generate spelling correction hypotheses.

\par
Besides the \Timbl{}-related functionality, \wopr{} can create and
manipulated data sets.

\chapter{Walk Throughs}

Without further ado, we present a number of common tasks which can be
performed with \wopr{}. We'll jump right in, and explain the different
steps and option on the way.

\section{Memory Based Language Model}

A memory based version of a trigram model. The following text corpora
are used in the experiments; \cmp{rmt.5e5} for our training data, and
\cmp{rmt.t1000} to test on. The former consists of the first
\num{500000} lines of the Reuters Newspaper corpus, while the latter
contains the last \num{1000} lines.

The following steps are typically taken when creating a model from
scratch.

\begin{varlist}{m}
\item[1] Create a lexicon
\item[2] Create a windowed data set (both for training and testing)
\item[3] Train the instance base
\item[4] Run a test on a test data set
\end{varlist}

The lexicon is created as follows.

\begin{bash}{Creating a lexicon}
wopr -r lexicon -p filename:rmt.5e5
\end{bash}

This will create a frequency list of all the tokens in the specified
file. Before we explain what we did, we will show the second step.

%\begin{lstlisting}[language=C,basicstyle=\ttfamily,numberstyle=\zebra{green!25}{green!2},numbers=left]
\begin{bash}{Creating a data set}
wopr -r window_lr -p filename:rmt.5e5,lc:2,rc:0
\end{bash}

These examples shows how \wopr{} is typically used. The two main
points to note are \cmp{-r window\_lr}, and the \cmp{-p \ldots}. The
\cmp{-r} tells \wopr{} to run the \cmp{window\_lr}
function. Everything that follows after \cmp{-p} are the parameters
passed to the function. The parameters are specified as a comma
seperated list of \cmp{keyword:value} pairs. In this case, we specify
a \cmp{filename}, \cmp{rmt.5e5} and the size of the left- and right
contexts. The context consists of two words on the left of the target,
and none on the right. This is abbreviated as \cmp{l2r0}.

\Wopr{} is quite verbose:

\begin{bash}{\Wopr{} output}
wopr -r window_lr -p filename:rmt.5e5,lc:2,rc:0
06:58:37.21: Timbl support built in.
06:58:37.22: /Users/pberck/local/
06:58:37.22: Starting wopr 1.29.3
06:58:37.22: PID:   7433 PPID:   2142
06:58:37.22: Starting.
06:58:37.22: Running: window_lr
06:58:37.22: window_lr
06:58:37.22:  filename:  rmt.5e5
06:58:37.22:  lc:        2
06:58:37.22:  rc:        0
06:58:37.22:  to:        0
06:58:37.22:  OUTPUT:    rmt.5e5.l2r0
06:59:15.61: SET filename to rmt.5e5.l2r0
06:59:15.61: Result = 0
06:59:15.61: Running for 38s
06:59:15.61: Ready.
\end{bash}

Note lines 13 and 14. In line 13, \wopr{} prints the name of the
output file it creates. These filenames are generated automatically by
\wopr{} and contain information about the parameters used. In the case
of \cmp{window\_lr}, it incorporates the context size in the file
name. This allows \wopr{} to skip steps in its processing if they have
been done before. Running the exact same command again will show the
following:

\begin{bash}{}
wopr -l -r window_lr -p filename:rmt.5e5,lc:2,rc:0
07:15:36.38: Running: window_lr
07:15:36.38: window_lr
07:15:36.38:  filename:  rmt.5e5
07:15:36.38:  lc:        2
07:15:36.38:  rc:        0
07:15:36.38:  to:        0
07:15:36.38:  OUTPUT:    rmt.5e5.l2r0
07:15:36.38: OUTPUT exists, not overwriting.
07:15:36.38: SET filename to rmt.5e5.l2r0
07:15:36.38: Result = 0
\end{bash}

Another thing to notice is the \cmp{SET} statement in line
\num{10}. This is useful when combining the different steps. This will
be explained later.

The next step is the creation of the instance base (output has been
reduced to improve readability).

\begin{bash}{Creating an instance base}
wopr -l -r make_ibase -p filename:rmt.5e5.l2r0,timbl:"-a4 +D"
10:02:16.99: Running: make_ibase
10:02:16.99: make_ibase
10:02:16.99:  timbl:     -a4 +D
10:02:16.99:  filename:  rmt.5e5.l2r0
10:02:16.99:  ibasefile: rmt.5e5.l2r0_-a4+D.ibase
...
Size of InstanceBase = 3366271 Nodes, (134650840 bytes), 29.46 % compression
Learning took 527 seconds, 732 milliseconds and 483 microseconds
Writing Instance-Base in: rmt.5e5.l2r0_-a4+D.ibase
10:12:57.87: SET ibasefile to rmt.5e5.l2r0_-a4+D.ibase
10:12:57.87: Result = 0
\end{bash}

Creating the instance based is delegated to \Timbl{}. We refer to
\cite{Daelemans+09} for a thorough explanation. The variables
specified in the \cmp{timbl} parameter are passed verbatim to \Timbl{}.

%The \cmp{\ldots} show where \Timbl{}, which is used by \wopr{} to
%train the instance base, prints its output.

And finally, we use this instance base to run a perplexity calculation
on a test set. The test set has been prepared in a step not shown
here, in the same format as the training data. It contains the last
\num{1000} lines of the Reuters data. Here we also specify the lexicon
created in step \textsf{1}. The lexicon is used to determine if a word
is known or unknown to the system.

\begin{bash}{Running a test}
wopr -l -r pplxs -p ibasefile:rmt.5e5.l2r0_-a4+D.ibase,
                    filename:rmt.t1000.l2r0,timbl:"-a4 +D",
                    lexicon:rmt.5e5.lex
...
11:50:10.93:  Correct:       3368 (20.5893)
11:50:10.93:  Correct Distr: 6250 (38.2076)
11:50:10.93:  Correct Total: 9618 (58.7969)
11:50:10.93:  Wrong:         6740 (41.2031)
11:50:10.93:  Timbl took: 11m47s
11:50:10.93:  SET px_file to rmt.t1000.l2r0_11615.px
11:50:10.93:  SET pxs_file to rmt.t1000.l2r0_11615.pxs
11:50:11.09: Result = 0
\end{bash}

\Wopr{} generates two output files. The important one is
\cmp{rmt.t1000.l2r0\_11615.px}. That contains the prediction and
statistics for each test instance we processed.

An extra perl script, \cmp{pplxs\_px.pl} has been supplied to post
process the output. It can be found in the \cmp{etc/} directory in the
\wopr{} distribution. It computes perplexity and other statistics on
the output. It is run as follows.

\begin{bash}{Post processing of \wopr{} output}
perl pplx_px.pl -f rmt.t1000.l2r0_11615.px -l2 -r0
\end{bash}

The file to process is specified with the \cmp{-f} parameter. The
context size is specified with the \cmp{-l} and \cmp{-r}
parameters. It contains a line for each instance, followed by a
summary. A fragment of the output:

\begin{wout}{}
0.02119694  -5.5600  -1.6737  1 1 [01010] ways
0.41176411  -1.2801  -0.3854  1 1 [01010] of
\end{wout}

It shows the probability of the classification, followed by the
$log_2$ and $log_{10}$ of the probability. For an explanation of the
other values, see the reference chapter. The summary at the end shows,
amongst other statistics, the perplexity on the test data.

\begin{wout}{}
Wordcount: 16358 sentencecount: 0 oovcount: 542
Wopr ppl:    251.41 Wopr ppl1:    251.41  (No oov words.)
\end{wout}

To get performance per word, the \cmp{-w} flag can be added.

\begin{wout}{}
winner: cg:0 (0.00%) cd:0 (0.00%) ic:1 (100.00%)
with: cg:11 (18.33%) cd:22 (36.67%) ic:27 (45.00%)
\end{wout}

This show that the word \cmp{winner} appeared once in the test set,
and was incorrectly predicted. The word \cmp{with} was predicted
correctly in \num{11} cases, \etc{}.

\subsection{Multiple Test Files}

Instead of processing just one file, a directory argument can be given
to \wopr{} to process all files matching a certain expression.

\begin{bash}{Processing multiple files}
wopr -l -r pplxs -p ibasefile:rmt.5e5.l2r0_-a4+D.ibase,
                    dir:expdir,timbl:"-a4 +D",
                    lexicon:rmt.5e5.lex
...
13:28:05.56:  dir:             expdir
13:28:05.56:  dirmatch:        .*
13:28:05.56: Processing 1 files.
...
13:28:34.65: Processing: expdir/rmt.t1000.l2r0
13:28:34.65: OUTPUT:     expdir/rmt.t1000.l2r0_4713.px
13:28:34.65: OUTPUT:     expdir/rmt.t1000.l2r0_4713.pxs
...
\end{bash}

Lines five and six show the directory, and the expression used to
match the files. The default is to take all the files in the
directory. To specify something else the parameter \cmp{dirmatch} can
be specified. To process all files ending in \cmp{l2r0}, the
expression \cmp{l2r0\$} can be supplied.

Line seven shows there is only one file, and the rest of the output
shows that each file is processed in a similar manner to the single
file method.

\subsection{Combining Commands}

As we mentioned before, the \wopr{}-commands can be combined. \Wopr{}
has a mechanism to automatically generate output filenames based on
certain parameters. It will \q{create} come of these parameters after
each run, so they will be available to the next function as input
without having to specify them from the start. We can combine several
of the steps we shows in the previous part like this.

\begin{bash}{Combining commands}
wopr -r lexicon,window_lr,make_ibase,pplxs -p filename:rmt.5e5,
                                              timbl:"-a4 +D",lc:2,rc:0
...
09:11:54.15: SET ibasefile to rmt.5e5.l2r0_-a4+D.ibase
09:11:54.15: Result = 0
09:11:54.15: Running: pplxs
09:11:54.15: pplxs
09:11:54.15:  ibasefile:      rmt.5e5.l2r0_-a4+D.ibase
09:11:54.15:  lexicon:        rmt.5e5.lex
09:11:54.15:  counts:         rmt.5e5.cnt
...
\end{bash}

If we look at the last step, \cmp{pplxs}, we see that the instance
base filename and the other parameters have been taken from the
previous steps. This example also shows theproblem with this
mechanism; we cannot generate the windowed test set in the same
chain. That has to be done seperately. (To be clear, one would not run
the example in real life. It was to show both the strength and
weakness of the combined commands.) For complex \wopr{} usage,
\emph{scripting} can (should) be used.

\section{\ngram{}-model}

\Wopr{} can also create a classical \ngram{}-model. This functionality
has been included so as to be able to compare the \mb{} \lm{} to a 
\q{pure} \ngram{}-model. It has neither been optimised for speed,
nor for memory efficiency.

Creation of the model consists of one step; \cmp{ngl}.

\begin{bash}{Creation of an \ngram{} model}
wopr -r ngl -p filename:rmt.5e5
11:19:40.16: Running: ngl
11:19:40.18: ngl
11:19:40.18:  filename:  rmt.5e5
11:19:40.18:  n:         3
11:19:40.18:  fco:       0
11:19:40.18:  OUTPUT:    rmt.5e5.ngl3f0
11:19:40.18: Reading...
11:21:33.15: Writing...
11:22:36.35: SET ngl to rmt.5e5.ngl3f0
11:22:45.65: Result = 0
\end{bash}

The output contains a list with all the \ngram{}s followed by a
frequency count and a conditional probability. The default \ngram{}
size is three.

The command to process a test file is called \cmp{ngt}.

\begin{bash}{testing with an \ngram{} model}
wopr -r ngt -p ngl:rmt.5e5.ngl3f0,testfile:rmt.t1000
11:31:50.24: Running: ngt
11:31:50.24: ngt
11:31:50.24:  filename:  rmt.t1000
11:31:50.24:  ngl file:  rmt.5e5.ngl3f0
11:31:50.24:  counts:    
11:31:50.24:  n:         3
11:31:50.24:  id:        
11:31:50.24:  OUTPUT:    rmt.t1000.ngt3
11:31:50.24:  OUTPUT:    rmt.t1000.ngp3
11:31:50.24: NOTICE: cannot read counts file, no smoothing will be applied.
11:31:50.24: P(0) = 1e-06
11:31:50.24: Reading ngrams...
11:31:59.71: Writing output...
11:32:00.02: Total words: 16358
11:32:00.02: Total oovs: 542
11:32:00.02: Total log2prob: -121551
11:32:00.02: Average log2prob: -7.6853
11:32:00.02: Average pplx: 205.828
11:32:00.02: SET ngt_file to rmt.t1000.ngt3
11:32:00.02: SET ngp_file to rmt.t1000.ngp3
11:32:01.90: Result = 0
\end{bash}

This produces two output files. One, \cmp{rmt.t1000.ngt3}, containing
statistics for each classifiation, as shown in the following fragment. It
shows a word, the \ngram{} probability, the size of the matching
\ngram{} and the \ngram{} itself.

\begin{wout}{}
and 0.0928793 2 puts and
calls 0.5 3 puts and calls
\end{wout}

The other, \cmp{rmt.t1000.ngp3}, contains statistics per line of text.

\begin{wout}{}
-81.9518 293.087 10 0 The officers quickly managed to calm down ...
\end{wout}

The output contains the entropy, perplexity, word count and number of
unknown words, followed by the text line itself.

\section{Text Generation}

\Wopr{} can also be used in reverse, and generate text from a
\lm{}. The command is called \cmp{generate}.

\begin{bash}{Text generation}
wopr -r generate -p ibasefile:rmt.5e5.l2r0_-a4+D.ibase,timbl:'-a4 +D',
                    filename:out1,ws:2
\end{bash}

The command still uses the older specification for context,
\cmp{ws}. It needs to be set to the sum of the left and right context
specifiers. 

\begin{wout}{}
Kodjo took the loss , no delays . 
USA : U.S. farm commodities to Russia , China . 
The state had to evacuate the church for two consecutive terms in an 
   abandoned car in the market moved in and out of parliament Koigi 
   wa Wamwere and two hawks on to Baltimore . 
The other 14 first list equities were unchanged in higher management
   ranks . 
\end{wout}


\section{Spelling Correction}

\Wopr{} can also be used to do spelling correction. The correction
algorithm is based on words in the predicted list of words. A filter
is applied to the list, removing words which fall outside predefined
parameters such as levenshtein distance, word length and frequency.

\begin{bash}{Spelling correction}
wopr -r correct -p ibasefile:rmt.5e5.l2r0_-a4+D.ibase,timbl:"-a4 +D",
                   filename:rmt.t1000.l2r0
14:16:50.07: Running: correct
14:16:50.07: correct
14:16:50.07:  ibasefile:  rmt.5e5.l2r0_-a4+D.ibase
14:16:50.07:  lexicon:    
14:16:50.07:  counts:     
14:16:50.07:  timbl:      -a4 +D
14:16:50.07:  id:         7272
14:16:50.07:  mwl:        5
14:16:50.07:  mld:        1
14:16:50.07:  max_ent:    5
14:16:50.07:  max_distr:  10
14:16:50.07:  min_ratio:  0
14:16:50.07: Processing 1 files.
...
14:17:13.02: Processing: rmt.t1000.l2r0
14:17:13.02: OUTPUT:     rmt.t1000.l2r0_7272.sc
...
14:35:42.79:  SET sc_file to rmt.t1000.l2r0_7272.sc
14:35:42.80: Result = 0
\end{bash}

The statistics printed (here removed) can be ignored. The output will
contain a list with the words falling within the parameters. They are
potential corrections of misspelt words.

Reuters doesn't contain misspellings, so we show output from another
file.

\begin{wout}{}
_ _ Her ssiter (sister) -6.26454 0 76.88 1 [ sister 1 ]
\end{wout}

It shows the instance, followed by the classifiers best guess between
parenthesis. This is followed by the logprob, the entropy and the word
logprob ($2^{-logprob}$). The final number is the size of the \Timbl{}
distribution returned, followed by a list with potential misspellings.

\section{Global Context}

\Wopr{} contains a mechanism to include a larger, \emph{global}
context in the classifier. This is done by marking words from a list
with words that are deemed important. When going through a text to
make instances, we put the words from the list we encounter in the
text in the global context. The global context has a certain size, so
that older words are pushed out by newer words. We also put a decay
factor on the words, so that they will disappear even if they are not
pushed out by newer words. These global context instances can be seen
as instances with gaps in them (after all, we don't include every
word), and the idea is that they capture the essence of the
(preceeding) text. The gaps are of variable length, so they have been
dubbed \emph{elastigrams}. To create the instances for \Timbl{}, we
add the global context instances to already existing \q{normal}
instances (in fact, the global context instances are created from a
regular data set).

There is a function to create the global context called
\cmp{lcontext}, and one to create a list with important words from the
lexicon, called \cmp{rfl}.

The steps involved are the following.

\begin{varlist}{m}
\item[1] create training data
\item[2] create a list
\item[3] create the global context from the list and training data
\end{varlist}

The first steps have been explained before, and will just be mentioned
here.

\begin{bash}{Creating lexicon and training data}
wopr -r lexicon -p filename:rmt.5e5
wopr -r window_lr -p filename:rmt.5e5,lc:2,rc:0
\end{bash}

In the next step, we create a list with words which we want to include
in our global context. We take a part of the lexicon, namely from the
50th most frequent word to the 550th most frequent word. We skip the
top-50 words because these are function words, and deemed less
important when trying to catch the essence of the text. The list will
still not be perfect, but serves our purpose.

\begin{bash}{Creating list for global context training data}
wopr -r rfl -p lexicon:rmt.5e5.lex,m:50,n:550
...
10:53:37.62: range_from_lex
10:53:37.62:  lexicon: rmt.5e5.lex
10:53:37.62:  m:       50
10:53:37.62:  n:       550
10:53:37.62:  OUTPUT:  rmt.5e5.lex.r50n550
10:53:37.62: Reading lexicon.
10:53:37.95: Read lexicon.
10:53:38.19: Frequency list: 1786 items.
10:53:38.25: Range file contains 539 items.
10:53:38.25: SET range to rmt.5e5.lex.r50n550
10:53:38.34: Result = 0
\end{bash}

The first five word in the file are:

\begin{wout}{}
year 17964
U.S. 17870
market 17175
but 16501
after 16004
\end{wout}

Now we can combine the list and the training data to form the global
context training data.

\begin{bash}{Creating global context training data}
wopr -r lcontext -p range:rmt.5e5.lex.r50n550,filename:rmt.5e5.l2r0,
                    gcd:50,gcs:10
...
10:58:39.91: lcontext
10:58:39.91: Reading range file.
10:58:39.91: Loaded range file, 539 items.
10:58:39.91:  filename:  rmt.5e5.l2r0
10:58:39.91:  range:     rmt.5e5.lex.r50n550
10:58:39.91:  gcs:       10
10:58:39.91:  gcd:       50
10:58:39.91:  gct:       0
10:58:39.91:  from_data: true
10:58:39.91:  gc_sep:    true
10:58:39.91:  OUTPUT:    rmt.5e5.l2r0.gc10d50t0
11:01:34.19: SET filename to rmt.5e5.l2r0.gc10d50t0
11:01:34.72: Result = 0
\end{bash}

The following shows a fragment of the resulting data set. In line two,
the word \cmp{markets} is (again) added to the head of the global context
part of the instance (fourth word from the right). In line three \cmp{into}
is added, and everything shifts a place to the left.

\begin{wout}{}
_ _ _ _ _ _ markets markets economy back sent Mexican markets
_ _ _ _ _ markets markets economy back markets Mexican markets into
_ _ _ _ markets markets economy back markets into markets into a
_ _ _ _ markets markets economy back markets into into a buzz
\end{wout}

Finally, we create a new instance base, and run a word prediction
experiment.

\begin{bash}{Creating global context training data}
wopr -r make_ibase -p filename:rmt.5e5.l2r0.gc10d50t0,timbl:'-a4 +D'
11:07:57.72: Timbl support built in.
11:07:57.72: /Users/pberck/local/
11:07:57.72: Starting wopr 1.29.3
11:07:57.72: PID:   9161 PPID:   4617
11:07:57.72: Starting.
11:07:57.72: Running: make_ibase
11:07:57.72: make_ibase
11:07:57.72:  timbl:     -a4 +D
11:07:57.72:  filename:  rmt.5e5.l2r0.gc10d50t0
11:07:57.72:  ibasefile: rmt.5e5.l2r0.gc10d50t0_-a4+D.ibase
...
11:20:12.39: SET ibasefile to rmt.5e5.l2r0.gc10d50t0_-a4+D.ibase
11:20:12.39: Result = 0
\end{bash}

\begin{bash}{Creating global context training data}
/exp/pberck/wopr/wopr -l -r pplxs -p filename:rmt.t1000.l2r0.gc10d50t0,
              timbl:"-a4 +D",ibasefile:rmt.5e5.l2r0.gc10d50t0_-a4+D.ibase,
              lexicon:rmt.5e5.lex
...
11:58:55.32: Processing: rmt.t1000.l2r0.gc10d50t0
11:58:55.32: OUTPUT:     rmt.t1000.l2r0.gc10d50t0_30826.px
11:58:55.32: OUTPUT:     rmt.t1000.l2r0.gc10d50t0_30826.pxs
12:51:30.18:  Correct:       3141 (19.2016)
12:51:30.18:  Correct Distr: 866 (5.29405)
12:51:30.18:  Correct Total: 4007 (24.4957)
12:51:30.18:  Wrong:         12351 (75.5043)
12:51:30.18:  Timbl took: 52m25s
12:51:30.18:  SET px_file to rmt.t1000.l2r0.gc10d50t0_30826.px
12:51:30.18:  SET pxs_file to rmt.t1000.l2r0.gc10d50t0_30826.pxs
12:51:30.24: Result = 0
\end{bash}

There are a number of parameters that can be tweaked to get a better
peformance. We refer to the reference section for details.

\section{Multi-classifiers}

\Wopr{} contains functionality to apply more than one classifier on a
task. One modus operandus is to apply multiple classifiers and combine
the different outputs, another is to use a gated approach, and choose
a certain classifier based on context features.

The instance bases and parameters are stored in a configuration file
which is read by \wopr{}.

\subsection{md/md2} %muldi_dist2: md2 and mc

%multi_dists, multi_classifiers?

The following shows a configuration file to apply two different
classifiers to a test set in two different formats (to match the
training set). The test sets need to be generated from the same data,
and so must the instance bases.

\begin{wout}{Example \cmp{kvs} file}%durian:multi pberck\$ more austenmd2.kvs
classifier:T0
ibasefile:austen.train.l2r0_-a1+D.ibase
timbl:-a1 +D
testfile:austen.test.l2r0

classifier:T1
ibasefile:austen.train.l3r0_-a1+D.ibase
timbl:-a1 +D
testfile:austen.test.l3r0
\end{wout}

\begin{bash}{}
wopr -r md2 -p kvs:austenmd2.kvs
10:21:33.93: Running: md2
...
10:21:33.93: Reading classifiers.
Reading Instance-Base from: austen.train.l2r0_-a1+D.ibase
Feature Permutation based on Data File Ordering :
< 2, 1 >
Reading weights from austen.train.l2r0_-a1+D.ibase.wgt
10:21:34.77: T0/1
10:21:34.77: T0/austen.train.l2r0_-a1+D.ibase/austen.test.l2r0/wgt=1/type=1
Reading Instance-Base from: austen.train.l3r0_-a1+D.ibase
Feature Permutation based on Data File Ordering :
< 3, 2, 1 >
Reading weights from austen.train.l3r0_-a1+D.ibase.wgt
10:21:36.13: T1/1
10:21:36.13: T1/austen.train.l3r0_-a1+D.ibase/austen.test.l3r0/wgt=1/type=1
10:21:36.13: Read classifiers. Starting classification.
10:21:52.30: T0: 1092/8308
10:21:52.31: T1: 1068/8308
10:21:52.31: SET filename to austenmd2.kvs_8713.mc
10:21:52.31: Result = 0
\end{bash}

Output:
\begin{wout}{}
Mrs [ and 0.0629821 1 true and 0.0629821 1 true ]
Dashwood [ Weston 0.138182 1 false Weston 0.138182 1 false ]
could [ was 0.113636 2 true and 0.133333 3 true ]
think [ find 1 2 true find 1 2 true ]
of [ of 0.192746 1 false of 0.705882 2 false ]
no [ it 0.125 2 true nothing 0.6 3 true ]
other [ consequence 0.192308 2 true other 1 3 true ]
\end{wout}

It shows the target followed by the output of each classifier between
square brackets. Each classification is followed by its probability,
which is calculated by taking the frequency of the answer divided by
the sum of all the frequencies in the distribution, the depth at which
\Timbl{} matched in the tree, and a boolean value indicating if we
matched at a leaf. Four values for each classifier.

\par
Extra parameter \cmp{c:1}, combines the distributions, and prints the
best of the combined between braces.

\begin{wout}{}
Mrs [ and 0.0629821 1 true and 0.0629821 1 true ] { and 0.125964 }
Dashwood [ Weston 0.138182 1 false Weston 0.138182 1 false ] { Weston 0.276364 }
could [ was 0.113636 2 true and 0.133333 3 true ] { was 0.24697 }
think [ find 1 2 true find 1 2 true ] { find 2 }
of [ of 0.192746 1 false of 0.705882 2 false ] { of 0.898628 }
no [ it 0.125 2 true nothing 0.6 3 true ] { nothing 0.634091 }
other [ consequence 0.192308 2 true other 1 3 true ] { other 1.07692 }
\end{wout}

Instead of applying several classifiers, a pre-made distribution can
be added to the classifiers output. Think of this as a way to
highlight/emphasise classes in the classifiers output. The extra
distribution file needs to contain as many instances as the regular
test set. We can, for example, create a file with global information
about each instance in the test file. When these occur in the
distribution of the classifier, they can be brought to the front.

An example \cmp{kvs}.

\begin{wout}{}%austen3.kvs
classifier:T0
ibasefile:austen.train.l2r0_-a1+D.ibase
timbl:-a1 +D
testfile:austen.test.l2r0

classifier:TG
distfile: austen.test.l0r0.gc10d50t0
distprob: 0.1
\end{wout}

Note that \wopr{} assumes that the data in the distfile is similar to a
data file with instances - that is, the last token on each line is
considered to be the target value and is ignored. The \cmp{\_} value is also
ignored, and doubles are removed.

Then we run \wopr{}.

\begin{bash}{}
wopr -r mc -p kvs:austen3.kvs,c:1,topn:5
\end{bash}

The output looks like this. Note that the output of the TG
"classifier" is not saved in the output file.

\begin{wout}{}
coolness [ a 0.181818 1 false ] { between 0.190909 each 0.190909 ... }
between [ of 0.25 1 false ] { between 0.35 arise 0.25 of 0.25 ... }
their [ the 0.214286 1 false ] { the 0.214286 them 0.193277 ... }
\end{wout}

Note the second line, where the combined distribution contains the
right answer.

\subsection{multi\_gated, mg (for wex)} %multi_gated: mg

\section{Server}

\Wopr{} can be run in \emph{server} mode where it will listen on a socket
for input and return a classification result.



\chapter{Installation}

\Timbl{} is required to run \wopr{}.

Assuming \Timbl{} is configured and installed locally with
\cmp{--prefix=/home/pberck/local}. We also install \wopr{} locally:

%\begin{lstlisting}[language=bash,basicstyle=\ttfamily,linebackgroundcolor={\ifodd\value{lstnumber}\color{green!25}\else\color{green!2}\fi}]
\begin{bash}{Configuring \wopr{} locally}
sh bootstrap
./configure --prefix=/home/pberck/local \
            --with-timbl=/home/pberck/local
make
make install
\end{bash}

If \Timbl{} is installed system wide, the \cmp{--with-timbl} parameter
can be omitted. The configure script should be able to figure out
where the libraries are. Wopr can be built without \Timbl{} by specifying
\cmp{--without-timbl}. This will leave you with a \wopr{} which can
create datafiles and run an \ngram{} model only.
%On Debian we need: \cmp{export LD\_LIBRARY\_PATH=\$LD\_LIBRARY\_PATH:/home/pberck/local/lib}
The \cmp{LD\_LIBRARY\_PATH} and \cmp{PATH} variables might need to be
set as well. A short test to see if all went well:

\begin{bash}{Running \wopr{}}
./wopr
10:44:32.19: Timbl support built in.
10:44:32.19: /Users/pberck/local/
10:44:32.19: Starting wopr 1.29.3
10:44:32.20: PID:   4173 PPID:   1136
10:44:32.20: Starting.
10:44:32.21: Running for 00s
10:44:32.21: Ready.
\end{bash}

The second line shows the path tot the \Timbl{} libraries.

\chapter{Performance}

\section{Memory}

\begin{comment}
// ---- 2011-06-13 15:47:50 Mon -----------------------------------------------
ceto
pberck@ceto:/exp2/pberck\$ /exp/pberck/wopr/wopr -l -r make_ibase -p filename:nyt.3e7.100000.l3r0,timbl:"-a1 +D"

and:
pberck@ceto:/exp/pberck/wopr/etc\$ bash watch_wopr.sh 30341 88G 10 1

gnuplot 30341_mem.plot
durian:doc pberck\$ scp -P223 pberck@portal.ticc.uvt.nl:doorgeefluik/303* .

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\pdfwidth]{30341_mem.pdf}
 \caption{Example gc instance base}
  \label{fig:30341_mem.pdf}
\end{figure}

// ---- 2011-06-13 15:55:26 Mon -----------------------------------------------

pberck@ceto:/exp2/pberck\$ wc -l nyt.3e7.1000000.l3r0                            22855429 nyt.3e7.1000000.l3r0

/exp/pberck/wopr/wopr -l -r make_ibase -p filename:nyt.3e7.1000000.l3r0,timbl:"-a1 +D"
bash watch_wopr.sh 32376 88G 10 1

testing with about ibase:
/exp/pberck/wopr/wopr -l -r pplxs -p
ibasefile:nyt.3e7.1000000.l3r0_-a1+D.ibase,timbl:"-a1+D",filename:lc/nyt.tail1000.l3r0
bash watch_wopr.sh  26251 88G 10 1
\end{comment}

Memory based classifiers tend to, nomen est omen, use a lot of
memory. To illustrate, we show two plots of memory usage; one for a
training task (\figurename~\ref{fig:32376_mem.pdf}) and one for a
classification task. The training corpus consisted of \num{1000000}
lines (\num{22855429} instances) of text taken from the Gigaword
Newspaper corpus. The context size was \cmp{l3r0}. The scale of the
$y$-axis in both graphs is in k\textsc{b}. The $x$-axis shows the time
taken in minutes.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\pdfwidth]{32376_mem.pdf}
 \caption{nyt.1000000.l3r0}
  \label{fig:32376_mem.pdf}
\end{figure}

The following plot (\figurename~\ref{fig:26251_mem.pdf}) shows the
memory use when using the above instance base for a classification
task. The test set consisted of \num{1000} lines of text from the same
Gigaword newspaper corpus.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\pdfwidth]{26251_mem.pdf}
 \caption{nyt.1000000.l3r0 ibase, test tail1000}
  \label{fig:26251_mem.pdf}
\end{figure}

\section{Comparison with \srilm{}}

The memory based approach to language modeling has several advantages
over a classical \ngram{}-based approach. First of all, we are not
limited to \ngram{}s but instead are able to use any number and type
of features in the instances. The instances can for example contain
grammatical and non-local context information. It is also possible to
use a right-context together, or even without, the words on the
left. 

Secondly, the tree-based approach has an inherent back-off
mechanism. In classical \ngram{}-models, a back-off to smaller
sequences (from trigrams to bigrams, to unigrams) is performed
when an \ngram{} is not found in the training data, typically when
unseen words are encountered. The \igtree{} algorithm returns the
distribution stored at that point in the tree when an unknown word is
encountered. The \triblt{} algorithm is more robust in that sense; it
continues to try to match the remaining feature values. 

In the following examples, we'll compare a \mb{} \lm{} with a \lm{}
generated by \srilm{}. 

\begin{comment}
We'll represent \wopr{} output like this;
an instance plus the target classification (in
grey), followed by the classification and distribution:\\ \\
  %\scalebox{0.95}{
\resizebox{!}{!}{ \wlr{w}{cg}{crazy
    when}{the}{}{the}{\dist{the}{526}\dist{he}{464}\dist{they}{219}\dsum{24}{2824}}}
\\The distribution is a list of tokens over their frequency. The top
figure after the $]$ shows the total number of elements in the
distribution. The bottom figure the sum of all the frequencies of the
distribution elements. In this example, \wopr{} predicted the correct
word \textsf{the} after \textsf{crazy when}. It returned a total of
\num{24} answers. The distribution can contain thousands of elements,
but we only show a handful of the top elements in our examples. If the
instance contains a right hand side context, it is shown after the
target. The next example shows \wopr{} output where the instance
contained right hand side context, and the wrong classification was
returned.\\ \\ \resizebox{!}{!}{\wlr{m}{ic}{crazy
    when}{the}{man}{a}{\dist{the}{488}\dist{a}{264}\dsum{8}{1244}}}\\An
incorrect classification is crossed out. A classification which is not
correct, but instead found in the distribution is marked with a dotted
line. 
%Finally, a \wex{} classification is marked with a small black
%dot in the upper left corner. Output from the \mono{} system is
%unmarked.
\end{comment}

\subsection{Logprob Comparisons}
%taken from personal/pberck/latex/wopr/worp_srilm.tex

We created two \lm{}s, one with \wopr{} and one with \srilm{}. We used
\num{10000000} lines of text from the Gigaword Newspaper corpus. For
\wopr{}, the context was set to \cmp{l3r0}. For \srilm{}, a fourth
order \lm{} was created. Test set was \num{100} lines taken from
the same corpus.

We take a sentence from the test data. First we show the \wopr{}
output. For each word in the test sentence we show the probability,
the logprob of the probability and the \wopr{} classification. The
logprob values have been converted to $log_{10}$.

\begin{wout}{\wopr{} output}
The         0.10521556   [ -0.97792002 ] ``
Carlyle     0.00000932   [ -5.03060257 ] company
Group       0.61538450   [ -0.21085345 ] Group
figures     0.00008171   [ -4.08771641 ] ,
prominently 0.00289761   [ -2.53795981 ] ,
in          0.88888889   [ -0.05115252 ] in
Unger's     0.00000004   [ -7.45497774 ] the
book        0.25000000   [ -0.60205999 ] book
.           0.04278088   [ -1.36875028 ] is
\end{wout}

And the \srilm{} output. The output is taken from the debug output from
\srilm{}. It shows the word (in context), followed by the size of the
matching \ngram{}, the probability and the $log_{10}$ of the probability.

\begin{wout}{\srilm{} output}
p( The | <s> )                = [2gram] 0.106276    [ -0.973567 ]
p( Carlyle | The ...)         = [3gram] 9.41261e-06 [ -5.02629 ]
p( Group | Carlyle ...)       = [4gram] 0.33338     [ -0.47706 ]
p( figures | Group ...)       = [1gram] 3.29037e-06 [ -5.48276 ]
p( prominently | figures ...) = [2gram] 0.00289762  [ -2.53796 ]
p( in | prominently ...)      = [3gram] 0.888889    [ -0.0511525 ]
p( Unger's | in ...)          = [1gram] 6.03086e-10 [ -9.21962 ]
p( book | Unger's ...)        = [2gram] 0.132676    [ -0.877207 ]
p( . | book ...)              = [2gram] 0.0525287   [ -1.2796 ]
\end{wout}

Note that \srilm{} wraps each sentence in a \emph{start of sentence}
and \emph{end of sentence} marker. For the sake of fairness, the
prediction of an end-of-sentence marker after a full stop has been
left out of the comparison. The start-of-sentence marker is a bit like
our empty context markers which we insert before the first word(s) in
a sentence. 

\par
\tablename~\ref{tab:logprob} shows the logprob values.

\begin{table}[h!]
\caption{Logprob values}
\centering
\vspace{\baselineskip}
\begin{tabular}{ l r r }
\toprule
Token & \wopr{} & \srilm{} \\ [.5ex]
\midrule
The         &     \num{-0.97792002}  & \bf{\num{-0.97356}\phantom{00}} \\
Carlyle     &     \num{-5.03060257}  & \bf{\num{-5.02629}\phantom{00}} \\
Group       & \bf{\num{-0.21085345}} &     \num{-0.47706}\phantom{00} \\
figures     & \bf{\num{-4.08771641}} &     \num{-5.48276}\phantom{00} \\
prominently & \bf{\num{-2.53795981}} & \bf{\num{-2.53796}\phantom{00}} \\
in          & \bf{\num{-0.05115252}} & \bf{\num{-0.0511525}} \\
Unger's     & \bf{\num{-7.45497774}} &     \num{-9.21962}\phantom{00}  \\
book        & \bf{\num{-0.60205999}} &     \num{-0.877207}\phantom{0} \\
.           &     \num{-1.36875028}  & \bf{\num{-1.2796}\phantom{000}} \\
\bottomrule
\end{tabular}
\label{tab:logprob}
\end{table}

Some predictions show a similar logprob from both \wopr{} and
\srilm{}. Both \lm{}s give the same likelyhood to the first word in
the sentence, \cmp{The}. Also the second word, \cmp{Carlyle} gets
similar logprobs. The next word shows a marked difference. \Srilm{}
matched a four-gram, that means the whole sequence \cmp{<s> The
  Carlyle Group} was found in the training data. The difference is due
to \emph{smoothing} and \emph{hapaxing}, which is done by \srilm{} but
not by \wopr{}.

\subsection{pplx}

The \igtree{} algorithm produces the following perplexity graph on the
English data (\figurename~\ref{fig:NY_pplx_-a1+D.pdf}).

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=\pdfwidth]{NY_pplx_-a1+D.pdf}
  \caption{Perplexity, \igtree{}}
  \label{fig:NY_pplx_-a1+D.pdf}
\end{figure*}

The perplexity score is low in the
beginning, due to the large number of unknown words. The only known
words are the function words, making up the skeleton of the
sentence. Most of the content words will still be unknown at this
point. As we train the classifier on more and more data, more of the
content words become known. This causes a rise in the perplexity score
first, before it starts to fall again.

The corresponding \srilm{} graph looks like this
(\figurename~\ref{fig:NY_pplx_slirm.pdf}). Note that \srilm{}
calculates two figures, one with (labeled pplx) and one without sentence
markers (labeled pplx1). 

\begin{figure*}[!ht]
  \centering         %gnuplot NY_pplx_srilm.plot
  \includegraphics[width=\pdfwidth]{NY_pplx_slirm.pdf}
  \caption{Perplexity, \srilm{}}
  \label{fig:NY_pplx_slirm.pdf}
\end{figure*}

\chapter{Reference}

\section{Data}

\subsection{\cmp{window\_lr}}

Creates a \emph{windowed} data set. The \cmp{lc} and \cmp{rc}
parameters set the size (number of words) of the left and right
context. Out of sentence feature values will be filled with as many
\cmp{\_} (underscore character) as needed. \Wopr{} assumes the input
file contains one sentence per line. It will slide a window over each
sentence, and each word will be at the target position once. There is
an extra parameter \cmp{to}, which when set will let you predict the
next word after the next word. That is, if set to \num{1}, it will not
take the token at the window position as target, but the one to the
right. Setting it to \num{2} skips one more, \etc{}. The default value
for \cmp{to} is 0 so it can be left out if so desired.

\begin{varlist}{filename}
\item[filename] The file to window.
\item[lc] Left context. Number of words on the left side of the target.
\item[rc] Right context. Number of words on the right side of the target.
\item[to] target offset. Shift the target position to the right. Setting this to
  one will skip one word, that is, a b -> d, c will be skipped.
\end{varlist}

The output filename will have the context appended to it, in the form
\cmp{l3r0}. The \cmp{to} parameter will be appended if non-zero.

\subsection{\cmp{hapax}}

Hapaxing translates all tokes with a frequency less than or equal to
the \cmp{hpx} parameter to the string \cmp{$<$unk$>$}. For hapaxing we use
a previously created lexicon on a \emph{windowed} data set.

\begin{varlist}{filename}
\item[filename] The windowed data set to hapax.
\item[lexicon] The lexicon file to use.
\item[hpx] The frequency value.
\item[hpx\_sym] The symbol to use for hapaxed tokens. Defaults to
  \cmp{$<$unk$>$}. 
\end{varlist}

\subsection{\cmp{lexicon}}

Creates a lexicon file for the given text file. The lexcion consists
of a list of tokens, followed by its frequency.

\begin{varlist}{filename}
\item[filename] The data set to read.
\end{varlist}

It creates three output files, one with the suffix \cmp{.lex}
containing the lexicon, and one with \cmp{.cnt} containing counts of
counts. The third file has the extension \cmp{.av} and contains a
anagram value for each word. The value is the sum over each letters'
\textsc{ascii} value to the power 5. This allows the rapid searching
of anagrams in a lexicon; the same letters will give the same value,
regardless of the order of the letters.

\begin{wout}{}
alerting 114508940294
altering 114508940294
integral 114508940294
relating 114508940294
triangle 114508940294
\end{wout}

\subsection{\cmp{ngl}}

This creates a file with uni- to \ngram{}s for the given text
file. For each \ngram{}, the absolute frequency count and a
conditional probability is calculated. The \cmp{fco} parameter is a
frequency cut off value; only \ngram{}s which occur more often than
the specified \cmp{fco} parameter are included in the model.

\begin{varlist}{filename}
\item[filename] The text file to read.
\item[n] The largest \ngram{} to include. Default is \cmp{3}.
\item[fco] The frequency cut-off value, defaults to \cmp{0}.
\end{varlist}

The output file will have the extension \cmp{.ngl3f0}, both the
\cmp{n} and \cmp{fco} value are included.

\subsection{\cmp{rfl}}

This command outputs a range of words taken from a lexicon. The
entries in the lexicon are sorted by frequency. The highest frequency
comes first and has rank 1.  Because we take ranks, a range of ten
ranks can contain more than ten items as a number of words can have
the same rank.

\begin{varlist}{lexicon}
\item[lexicon] The lexicon file to process.
\item[n] Starting rank.
\item[m] Last rank.
\end{varlist}

\subsection{\cmp{lcontext}}

Create the global context part of the instances, and optionally merge
them with a normal data set.

\begin{varlist}{lexicon}
\item[filename] The normal data set or text file, depending on the
  value of the \cmp{gct} parameter.
\item[range] The file containing the words for the global context.
\item[gcs] Global context size
\item[gcd] Global context decay
\item[gct] Global context type. Type \cmp{0}, the default, creates a
  global context consisting of words. Type \cmp{1} creates binary
  features. The latter means a binary value for every word in the
  \cmp{range} list supplied. The \cmp{gcs} parameter is ignored in
  this case. Type \cmp{2} creates a \q{hashed} value, that is, one
  value representing the words in the global context, in a position
  independent order.
\item[fd] Create from data (default \cmp{1}). Takes the words from a
  normal, windowed data set. Setting this to \cmp{0} creates global
  context from a normal text file.
\item[id] Identifier to append to the output filenames. If not
  specified, it defaults to the empty string.
\item[gc\_sep] Defaults to \cmp{1}, a space. This space is inserted
  between the binary (type \cmp{1}) feature values. Setting it to
  \cmp{0} concatenates all the binary features into one binary string
  feature. 
\end{varlist}

\subsection{\cmp{gap}}

A function to examine how words are distributed in a text. Can even be
used to generate data for the \emph{global context} data.

This function counts the number of tokens between the occurrences of
the words we are examining. A \cmp{gap} parameter is used to determine
whether words are \q{close} together or not. Two output files are
produced, one with suffix \cmp{gap} and one with suffix \cmp{gs}. The
gap size is appended to the suffixes. The first file contains a list
with words plus the gaps and a few statistics. Groups of small gaps
are enclosed between parenthesis, giving an idea of how the words are
clustered. The second file contains just the words and the statistics,
and can optionally be filtered on several parameters.

An example of the contents of the first file.

\begin{wout}{\cmp{gap} output}
wines 13 ( 51 93 ) 396 ( 101 39 29 38 17 78 11 22 16 ) 
                             [ 2 3 0.666667 11 1 0.916667 495 41.25 1.25 ]
\end{wout}

The example shows the token \cmp{wines}, which occurs \num{14} times
in the text. The first three (note that we show \emph{gaps}) are
\num{51} and \num{93} tokens apart. Then there is a gap of \num{396}
tokens until we come across the next occurrence of \cmp{wines}. Then
there is another group of \num{10} occurrences close together.

The first two numbers in the statistics show the number of close
groups, and the number of potential groups. We have the two bracketed
groups represented by the first number. There could have been three
groups if the large gap in the middle had had an occurrence of
\cmp{wines} within the gap distance, so that makes the total
three. The number after that shows the ratio between those two
numbers. The next numbers, \num{11} and \num{1}, show we have \num{11}
close gaps and only one large gap. The ratio between the number of
close gaps and the total number of gaps is shown by the next
number. Note that with \num{13} occurrences of the token, we only have
\num{12} gaps. The number \num{495} is the sum of the close gaps
values, which in its turn is followed by the average close gap
distance. The last number is the ratio between the average small gap
size and the average large gap size.

The second file contains the same information as the first one, but
without the gaps.

\begin{wout}{\cmp{gs} output}
wines 13 2 3 0.666667 11 1 0.916667 495 41.25 1.25
\end{wout}

This list can be filtered by setting the \cmp{filter} parameter to
\cmp{1}. Only tokens satisfying the conditions will be added to the
\cmp{gs} file. The \cmp{gap} file will still contain all the tokens
from the specified lexicon.

\begin{varlist}{lexicon}
\item[filename] A normal text file.
\item[lexicon] List of words to examine, in \wopr{}s lexicon format.
\item[gap] The maximum gap between tokens to consider them
  \q{together}.
\item[filter] A boolean to specify if we want to filer the \cmp{.gs}
  output. If set, the following conditions must be met for a word
  to be included in the output.
\item[min\_f] Minimum word frequency of words to include. Defaults to
  0. The token frequency must be larger or equal to this.
\item[max\_f] Maximum word frequency of words to include. Defaults to
  the maximum \emph{long} value the computer can handle. The token
  frequency must be less than this.
\item[min\_r] Minimum ratio between small and large gaps. Defaults to
  0.5. The ratio must be larger or equal to this.
\item[max\_r] Maximum ratio between small and large gaps. Defaults to
  1.1. It can never be more than 1, but the default will make sure it
  will be included. The ratio has to be less that this parameter.
\item[min\_a] Minimum average small gap value.
\item[max\_a] Maximum average small gap value.
\item[min\_g] Minimum groups to potential groups ratio.
\item[max\_g] Maximum groups to potential groups ratio.
\end{varlist}

%woordn^5  =>?  woordx^5 + woordy^5 (collision)

\section{Training}

\subsection{make\_ibase}

\begin{varlist}{filename}
\item[filename] Filename of the windowed data set.
\item[timbl] Settings for \Timbl{}.
\end{varlist}

\subsection{ngl}

Create an \ngram{} model.

\begin{varlist}{filename}
\item[filename] Filename of the plain text data.
\item[n] The $n$ in \ngram{}.
\item[fco] Frequency cut-off; \ngram{}s with a frequency below the
  value of the \cmp{fco} parameter are excluded from the model.
\end{varlist}


\section{Testing}

\subsection{\cmp{pplxs}}

The \cmp{pplxs} function is geared towards word prediction. It takes
an instance base, and calls \Timbl{} to process the test file. A
number of word prediction related values are calculated and written to
the output file.

It creates two output files, one with suffix \cmp{.px} and one with
suffix \cmp{.pxs}.  The \cmp{.px} output contains the following:

\begin{wout}{}
85 bp bp 0 0 1 cg k 1 1 1 11 1 [ bp 11 ]
bp over over 0 0 1 cg k 1 1 1 11 1 [ over 11 ]
over LIBOR Libor -16.4148 0 87364.5 ic k 1 1 1 11 0 [ Libor 11 ]
LIBOR delayed . -13.915 2.502 15452.2 ic k 1 1 8 44 0 [ . 14 , 11 for 6 ]
\end{wout}

The first tokens contain the instance (in this case \cmp{l1r0}),
followed by the classification. This is followed by the logprob of the
classification, the entropy of the distribution and the word level
perplexity (\wlp{}). The logprob is the $log_2$ of the probability of the
classification. The entropy of the distribution $D$ is calculated as
follows:

\begin{equation}
H(p) = -\sum_{x \in D} \, p(x)\log_2 p(x)
\label{eq:entropy}
\end{equation}

The word level perplexity is defined as:

\begin{equation}
\wlp{} = 2^{-p_{classification}}
\label{eq:wlp}
\end{equation}

These three numbers are followed by an indicator (\cmp{cg}, \cmp{cd}
or \cmp{ic}), followed by an known/unknown word indicator
(\cmp{k}/\cmp{u}). This is followed by the match-depth and a boolean
value indicating matched-at-leaf (one or zero). See
\cite{Daelemans+09} for a more thorough explanation. The following
three values are the distribution size, the sum of the distribution
frequencies, and the \emph{reciprocal rank} of the answer. Finally,
the output contains the top-$n$ of the distributions, specified as a
list of \cmp{token frequency} pairs.

The \cmp{.pxs} output contains a summary per line of input. It
contains the sentence number, followed by the number of words in the
sentence, the sum of the logprobs of the words, the average
perplexity, and the average \wlp{}. This is followed by
the number of unknown words in the sentence, the sum of the logprobs
of the known words and the standard deviation of the word level
perplexities. The final element in the output is a list with the
\wlp{} of each word in the sentence.

\begin{wout}{}
0 14 -113.718 278.73 30724.6 14 -113.718 75772.5 [ 594.087 105.375
                     181718 28.3925 392.646 116.143 1.61151 216331
                     2616.16 28.3925 28129.8 68.2397 2 11.6667 ]
\end{wout}

\begin{varlist}{ibasefile}
\item[filename] Test file to process.
\item[dir] Instead of one file, a whole directory with files will be
  processed if the \cmp{dir} parameter is specified instead of the
  \cmp{filename} parameter.
\item[dirmatch] A regular expression which determines which files will
  be read from the directory specified with \cmp{dir}.
\item[ibasefile] Filename of the instance base.
\item[timbl] Settings used to create the instance base.
\item[lexicon] Lexicon to determine known/unknown words.
\item[counts] Counts of ranks for the lexicon. This can be used to
  introduce smoothed or changed probabilities to the perplexity
  calculations. 
\item[hapax] Used when reading the lexicon.
\item[lc] Used to determine when a new sentence starts (for the
  \cmp{.pxs} output).
\item[rc] Used to determine when a new sentence starts (for the
  \cmp{.pxs} output). 
\item[topn] Number of elements from the \Timbl{} distribution to
  include in the output. Most frequent one first.
\item[cache] Number of elements in the cache (the default of three is
  good). 
\item[cth] Cache threshold: A distribution is not cached until it
  contains this many items.
\item[is] Include the whole sentence in the output (pxs).
\item[id] Identifier to append to the output filenames. If not
  specified, it defaults to the \pid{}.
\end{varlist}

\subsection{\cmp{gt}}

The \cmp{gt} function is a general test routine; it calls \Timbl{} and
writes the output to a file. The word prediction calculations are not
performed. The output filename is the name of the testfile, followed
by the \cmp{id} and the suffix \cmp{gt}.

The output contains the following:

\begin{wout}{}
million million cg 1 1 20 224 [ million 139 mln 30 billion 11 ]
\end{wout}

The first token is the target, the second the classification from
\Timbl{}. This is followed by an indicator (cg, cd, or ic), followed
by two values which indicate the match-depth, and a boolean value
specifying matched-at-leaf (one or zero). If the \cmp{topn} parameter
had been specified, this is followed by the distribution count, the
sum of the frequency of the elements in the distribution, and the
top-$n$ from the distribution. The latter is a list with
\cmp{token frequency} pairs.

\begin{varlist}{ibasefile}
\item[filename] Test file to process.
\item[dir] Instead of one file, a whole directory with files will be
  processed if the \cmp{dir} parameter is specified instead of the
  \cmp{filename} parameter.
\item[dirmatch] A regular expression which determines which files will
  be read from the directory specified with \cmp{dir}.
\item[ibasefile] Filename of the instance base.
\item[timbl] Settings used to create the instance base.
\item[cache] Number of elements in the cache (the default of three is
  good). 
\item[cth] Cache threshold: A distribution is not cached until it
  contains this many items.
\item[cs] Size of the cache which caches test instances. Default is
  \num{10000}. Note that this is a different cache from the cache which
  caches distributions. 
\item[topn] Number of elements from the \Timbl{} distribution to
  include in the output. Most frequent one first.
\item[id] Identifier to append to the output filenames. If not
  specified, it defaults to the \pid{}.
\end{varlist}

\subsection{\cmp{ngt}}

Apply an \ngram{} model to the test set. It finds the longest matching
\ngram{} from the supplied \ngram{}-model that fits.

\begin{varlist}{filename}
\item[filename] Filename of the plain text file to apply the model to.
\item[ngl] The \ngram{} model.
\item[counts] File with counts.
\item[n] Only \ngram{}s up to and including $n$ are used.
\item[id] Identifier to append to the output filenames. If not
  specified, it defaults to the empty string.
\end{varlist}

It creates two output files, one with the extension \cmp{.ngp3}, and
one with the extension \cmp{.ngp3}. The \cmp{3} refers to the
$n$-parameter supplied. The \cmp{.ngt} output contains information per
word in the text. The \cmp{.ngp} file contains statistics per line of
text. 

\subsection{\cmp{correct}}

Word correction based on contents of the distribution returned by the
classifier.

\begin{varlist}{max\_distr}
\item[filename] Test file to process.
\item[dir] Instead of one file, a whole directory with files will be
  processed if the \cmp{dir} parameter is specified instead of the
  \cmp{filename} parameter.
\item[dirmatch] A regular expression which determines which files will
  be read from the directory specified with \cmp{dir}.
\item[ibasefile] Filename of the instance base.
\item[timbl] Settings used to create the instance base.
\item[lexicon] Lexicon to determine known/unknown words.
\item[counts] Counts of ranks for the lexicon. This can be used to
  introduce smoothed or changed probabilities to the perplexity
  calculations. 
\item[mwl] Minimum word length (guess added if > mwl). Default 5.
\item[mld] Maximum levenshtein distance (guess added if <=
  mld). Default 1. Setting this to a larger value will introduce many
  potential misspellings.
\item[max\_ent] Max entropy of the distribution (guess added if <=
  max\_entropy). Default 5.
\item[max\_distr] Maximum size of the distribution (guess added if <=
  max\_distr). Default 10.
\item[min\_ratio] Ratio of the target lexical frequency versus the
  frequency of the words in the distribution. This is geared towards
  the disambiguation of confusibles and correctly spelt words on
  levenshtein distance one (like \cmp{woman} versus
  \cmp{women}). Default 0 (ignored).
\item[id] Identifier to append to the output filenames. If not
  specified, it defaults to the \pid{}.
\end{varlist}


\section{Miscellaneous}

\subsection{\cmp{generate}}

\begin{varlist}{ibasefile}
\item[start] Fill the starting context with this string. The default
  is to start generating from an empty context. It needs to contain as
  many words as specified in the \cmp{ws} parameter.
\item[filename] Output filename prefix.
\item[ibasefile] The instance base.
\item[timbl] The corresponding \Timbl{} settings.
\item[end] The end of sentence marker, defaults to a full stop.
\item[ws] The size of the left and right context added together.
\item[mode] A parameter to control if the next word taken from a
  number of possibilities is weighted according to frequency. Default
  is on (\cmp{1}). Setting it to \cmp{0} gives equal probability to
  each possibility.
\item[sc] Show the frequency count of each word.
\item[len] The maximum length of each sentence if the end of sentence
  marker is not reached.
\item[n] The number of sentences to generate.
\end{varlist}

\bibliographystyle{apalike}
\bibliography{ilk}

\end{document}


